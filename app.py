# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EJlVQ4ke-DikPqDkXKKKpIeL4mE2LNo1

# **Dashboard Analisis Sentimen Komentar YouTube**
Aplikasi ini digunakan untuk menganalisis sentimen komentar YouTube (Positif & Negatif)
menggunakan model **TF-IDF + XGBoost** yang telah dilatih sebelumnya.

# **IMPORT LIBRARY**
"""

import pandas as pd
import numpy as np
import re, emoji, joblib
import matplotlib.pyplot as plt


from googleapiclient.discovery import build
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import nltk

nltk.download('stopwords')

"""# **LOAD MODEL & TF-IDF**"""

model = joblib.load("model_xgboost_sentiment.pkl")
tfidf = joblib.load("tfidf_vectorizer.pkl")

"""# **PREPROCESSING**"""

stop_words = set(stopwords.words("indonesian"))
stemmer = StemmerFactory().create_stemmer()

normalisasi_dict = {
    "gk": "tidak", "ga": "tidak", "ngga": "tidak",
    "yg": "yang", "d": "di", "klo": "kalau",
    "gw": "saya", "gue": "saya", "km": "kamu", "tp": "tapi"
}

def preprocess_text(text):
    text = str(text).lower()
    text = emoji.replace_emoji(text, "")
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"[^a-z\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    tokens = text.split()
    tokens = [normalisasi_dict.get(t, t) for t in tokens]
    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]
    tokens = [stemmer.stem(t) for t in tokens]

    return " ".join(tokens)

"""# **FUNGSI EKSTRAK VIDEO ID**"""

def extract_video_id(youtube_url):
    pattern = r"(?:v=|\/)([0-9A-Za-z_-]{11})"
    match = re.search(pattern, youtube_url)
    if match:
        return match.group(1)
    else:
        return None

"""# **FUNGSI AMBIL KOMENTAR YOUTUBE**"""

API_KEY = "AIzaSyCz4yYoK_w-3IYkbeGCHtL4CoATBf6VN1I"

def get_comments(video_id, max_results=500):
    youtube = build("youtube", "v3", developerKey=API_KEY)

    comments = []
    next_page_token = None

    while len(comments) < max_results:
        response = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token,
            textFormat="plainText"
        ).execute()

        for item in response["items"]:
            comments.append(
                item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            )

        next_page_token = response.get("nextPageToken")
        if not next_page_token:
            break

    return comments

"""# **INPUT LINK YOUTUBE**"""

def extract_video_id(url):
    if "youtu.be/" in url:
        return url.split("youtu.be/")[1].split("?")[0]
    elif "v=" in url:
        return url.split("v=")[1].split("&")[0]
    else:
        return None

video_link = input("Masukkan link YouTube: ")
video_id = extract_video_id(video_link)

if video_id is None:
    raise ValueError("Link YouTube tidak valid")

"""# **AMBIL & PREPROCESS KOMENTAR**"""

comments = get_comments(video_id, max_results=500)

df = pd.DataFrame(comments, columns=["comment"])
df["clean_comment"] = df["comment"].apply(preprocess_text)

X = tfidf.transform(df["clean_comment"])
df["label"] = model.predict(X)
df["sentiment"] = df["label"].map({1: "Positif", 0: "Negatif"})

df.head()

"""# **DISTRIBUSI SENTIMEN**"""

df["sentiment"].value_counts().plot.pie(
    autopct="%1.1f%%",
    figsize=(5,5),
    title="Distribusi Sentimen Komentar"
)
plt.ylabel("")
plt.show()

"""# **TOP 5 KOMENTAR POSITIF & NEGATIF**"""

print("TOP 5 KOMENTAR POSITIF")
display(df[df["sentiment"]=="Positif"]["comment"].head(5))

print("\nTOP 5 KOMENTAR NEGATIF")
display(df[df["sentiment"]=="Negatif"]["comment"].head(5))